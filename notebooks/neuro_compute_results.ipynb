{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use cpu.\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "use_cuda = True\n",
    "if torch.cuda.is_available() and use_cuda:\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Use cuda.\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Use cpu.\")\n",
    "to_dev = lambda arr: (torch.from_numpy(arr).to(device)\n",
    "                       if type(arr) == np.ndarray else arr.to(device))\n",
    "\n",
    "# Data path\n",
    "from specs import data_path\n",
    "\n",
    "from helpers import gen_corr, ridge_CCA\n",
    "from rnn_model_dt import RNN_Net, find_fp\n",
    "from task_generators import cycling, flipflop, mante, romo, complex_sine\n",
    "task_gens = [cycling, flipflop, mante, romo, complex_sine]\n",
    "\n",
    "# Compute correlation\n",
    "task_names = [\n",
    "    \"cycling\",\n",
    "    \"flipflop\",\n",
    "    \"mante\",\n",
    "    \"romo\",\n",
    "    \"complex_sine\",\n",
    "]\n",
    "task_lbls = [\n",
    "    \"Cycling\",\n",
    "    \"3-bit flipflop\",\n",
    "    \"Mante\",\n",
    "    \"Romo\",\n",
    "    \"Complex sine\",\n",
    "]\n",
    "n_task = len(task_names)\n",
    "file_names = []\n",
    "for tn in task_names:\n",
    "    file_name = \"neuro_noisy_\" + tn + \"_n_512\" + \".pkl\"\n",
    "    file_names.append(file_name)\n",
    "    \n",
    "\n",
    "# Whether to add neural noise \n",
    "with_noise = False\n",
    "\n",
    "#######################################################################################\n",
    "# Number of samples\n",
    "n_samples = 5\n",
    "# Number of scenarios = weight initializations\n",
    "n_sce = 4\n",
    "n_mi = n_samples, n_sce\n",
    "\n",
    "# Grace time: Minimal time for states and output to be taken into account\n",
    "t_pc_min = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cycling\n",
      "Loaded from  ../data/neuro_noisy_cycling_n_512.pkl\n",
      "Took 68.0 sec.\n",
      "flipflop\n",
      "Loaded from  ../data/neuro_noisy_flipflop_n_512.pkl\n",
      "Took 20.6 sec.\n",
      "mante\n",
      "Loaded from  ../data/neuro_noisy_mante_n_512.pkl\n",
      "Took 27.5 sec.\n",
      "romo\n",
      "Loaded from  ../data/neuro_noisy_romo_n_512.pkl\n",
      "Took 20.0 sec.\n",
      "complex_sine\n",
      "Loaded from  ../data/neuro_noisy_complex_sine_n_512.pkl\n",
      "Took 57.3 sec.\n",
      "Saved to  ../data/neuro_corr_regression_no_noise.pkl\n"
     ]
    }
   ],
   "source": [
    "# Compute correlation and regression of output. Takes\n",
    "# Takes about 4 min.\n",
    "\n",
    "# Correlation\n",
    "loss_sce = []\n",
    "n_steps_sce = []\n",
    "lr0s_sce = []\n",
    "w_keys = [\"rnn.weight_ih_l0\", \"rnn.weight_hh_l0\", \"decoder.weight\"]\n",
    "n_ws = len(w_keys)\n",
    "n_ifd = 3\n",
    "norm_w_sce = np.zeros((n_task, *n_mi, n_ws, n_ifd))\n",
    "norm_h_sce = np.zeros((n_task, *n_mi, n_ifd))\n",
    "corr_w_h_sce = np.zeros((n_task, *n_mi, n_ifd, 2))\n",
    "\n",
    "# Number of PCs to be regressed on.\n",
    "n_comp_max = 30\n",
    "n_comps = np.arange(n_comp_max)+1\n",
    "n_nc = len(n_comps)\n",
    "# Subset of outputs used for training / testing\n",
    "frac_train = 3/4\n",
    "# Possible rgularization parameters\n",
    "alpha_range = np.logspace(-3, 6, 20)\n",
    "# Results\n",
    "loss = torch.zeros((n_task, *n_mi))\n",
    "loss_fit = torch.zeros((n_task, *n_mi, n_nc))\n",
    "loss_test = torch.zeros((n_task, *n_mi, n_nc))\n",
    "r_sq = np.zeros((n_task, *n_mi, n_nc))\n",
    "r_sq_test = np.zeros((n_task, *n_mi, n_nc))\n",
    "alphas = np.zeros((n_task, *n_mi, n_nc))\n",
    "bs_train_all = np.zeros((n_task), dtype=int)\n",
    "cevr_fit = np.zeros((n_task, *n_mi, n_nc))\n",
    "\n",
    "# Iterate over tasks\n",
    "loss_crit = torch.nn.MSELoss()\n",
    "for i_task in range(n_task):\n",
    "    task_name = task_names[i_task]\n",
    "    file_name = file_names[i_task]\n",
    "    data_file = data_path + file_name\n",
    "    print(task_name)\n",
    "    with open(data_file, 'rb') as handle:\n",
    "        res = pickle.load(handle)\n",
    "    [\n",
    "        n_steps, n_samples, gs, out_scales, n_sce, opt_gens, lr0s, n_mi, dim_hid, dim_in, dim_out, \n",
    "        dt, rec_step_dt, n_layers, bias, train_in, train_hid, train_out, train_layers, nonlin, \n",
    "        gaussian_init, h_0_std, noise_input_std, noise_init_std, noise_hid_std, batch_size, \n",
    "        task_params, task_params_ev, n_t_ev, task_ev, n_if, n_ifn, steps, loss_all, \n",
    "        output_all, hids_all, \n",
    "        h_0_all, sd_if_all, \n",
    "        ] = res[:38]\n",
    "    del res\n",
    "    print('Loaded from ', data_file)\n",
    "    n_t_min = int(t_pc_min / (dt * rec_step_dt))\n",
    "    ts_ex, input_ex, target_ex, mask_ex, noise_input_ex, noise_init_ex = [to_dev(arr) for arr in task_ev]\n",
    "    \n",
    "    # Only keep the hidden states necessary\n",
    "    if with_noise:\n",
    "        # Dynamics with the noise used during training.\n",
    "        hids_init_all = hids_all[0]\n",
    "        hids_final_all = hids_all[1]\n",
    "        output_final_all = output_all[1]\n",
    "    else:\n",
    "        # Noise-free testing dynamics: i_ifn = 2, 3\n",
    "        hids_init_all = hids_all[2]\n",
    "        hids_final_all = hids_all[3]\n",
    "        output_final_all = output_all[3]\n",
    "    del hids_all, output_all\n",
    "\n",
    "    ################################################################################\n",
    "    # Change in weights\n",
    "    for mi in np.ndindex(*n_mi):\n",
    "        for i_w, key in enumerate(w_keys):\n",
    "            w_init = sd_if_all[0][mi][key]\n",
    "            w_final = sd_if_all[1][mi][key]\n",
    "            dw = w_final - w_init\n",
    "            for i_if, w_i in enumerate([w_init, w_final, dw]):\n",
    "                norm_w_sce[i_task][mi][i_w, i_if] = torch.sqrt((w_i**2).mean())\n",
    "\n",
    "    #################################################################################\n",
    "    # Activity and correlation to weights\n",
    "    for mi in np.ndindex(*n_mi):\n",
    "        hids_init = hids_init_all[mi]\n",
    "        hids_final = hids_final_all[mi]\n",
    "        # Difference. Note that the noise is the same for both init and final!\n",
    "        d_hids = hids_final - hids_init\n",
    "        for i_h, hids_i in enumerate([hids_init, hids_final, d_hids]):\n",
    "            # Discard some initial dynamics\n",
    "            hids_i = hids_i[:, n_t_min:]\n",
    "            # Norm\n",
    "            norm_h_sce[i_task][mi][i_h] = torch.sqrt((hids_i**2).mean())\n",
    "            # Correlation to output weights\n",
    "            i_if = [0, 1, 1][i_h]\n",
    "            w_out = sd_if_all[i_if][mi][\"decoder.weight\"]\n",
    "            corr_w_h_sce[i_task][mi][i_h] = gen_corr(w_out, hids_i)\n",
    "\n",
    "    time0 = time.time()\n",
    "    for mi in np.ndindex(*n_mi):\n",
    "        i_s, i_sce = mi\n",
    "        \n",
    "        # Select states and output\n",
    "        hids = hids_final_all[i_s, i_sce]\n",
    "        output = output_final_all[i_s, i_sce]\n",
    "        \n",
    "        # Regress output from hidden states after PC (trail-cond average)\n",
    "        # Run PCA\n",
    "        pca = PCA(n_comp_max)\n",
    "        h = hids[:, n_t_min:, :].reshape(-1, dim_hid)\n",
    "        pca.fit(h)\n",
    "        h_proj = pca.transform(hids.reshape(-1, dim_hid)).reshape(batch_size, -1, n_comp_max)\n",
    "        cevr_fit[i_task][mi] = pca.explained_variance_ratio_.cumsum()\n",
    "        # Regression on the activity projected on the PCs\n",
    "        bs_train = int(frac_train * batch_size)\n",
    "        bs_train_all[i_task] = bs_train\n",
    "        y_train = output[:bs_train].reshape(-1, dim_out)\n",
    "        for i_nc, n_comp in enumerate(n_comps):\n",
    "            X_train = h_proj[:bs_train, :, :n_comp].reshape(-1, n_comp)\n",
    "            ridge = RidgeCV(alphas=alpha_range).fit(X_train, y_train)\n",
    "            # Output for the entire batch (separate train and test later)\n",
    "            output_fit = ridge.predict(h_proj[:, :, :n_comp].reshape(-1, n_comp)).reshape(batch_size, -1, dim_out)\n",
    "            output_fit = torch.from_numpy(output_fit.astype('float32'))\n",
    "            # Compute loss\n",
    "            loss_fit[i_task][mi][i_nc] = loss_crit(output_fit[mask_ex], target_ex[mask_ex])\n",
    "            loss_test[i_task][mi][i_nc] = loss_crit(output_fit[bs_train:][mask_ex[bs_train:]], target_ex[bs_train:][mask_ex[bs_train:]])\n",
    "            r_sq[i_task][mi][i_nc] = 1 - ((output - output_fit)**2).mean()  / ((output - output.mean((0, 1), keepdims=True))**2).mean()\n",
    "            r_sq_test[i_task][mi][i_nc] = 1 - ((output - output_fit)[bs_train:]**2).mean()  / ((output - output.mean((0, 1), keepdims=True))[bs_train:]**2).mean()\n",
    "            alphas[i_task][mi][i_nc] = ridge.alpha_\n",
    "\n",
    "    print(\"Took %.1f sec.\"% (time.time() - time0))\n",
    "        \n",
    "lbl_sce = [r\"$g=%.1f$, $\\sigma^{(0)}$ %s\" % (gs[i_sce], out_scales[i_sce]) \n",
    "           for i_sce in range(n_sce)]\n",
    "res = [\n",
    "    task_names, task_lbls, n_task, lbl_sce,\n",
    "    n_samples, n_sce, n_mi, dim_hid, \n",
    "    with_noise, loss_sce, n_steps_sce, lr0s_sce, \n",
    "    norm_w_sce, norm_h_sce, corr_w_h_sce, \n",
    "    n_comp_max, n_comps, n_nc, frac_train, alpha_range, bs_train_all, \n",
    "    loss, loss_fit, loss_test, r_sq, r_sq_test, alphas, cevr_fit,\n",
    "]\n",
    "file_name = \"neuro_corr_regression\"\n",
    "if not with_noise:\n",
    "    file_name += \"_no_noise\"\n",
    "file_name = \"_\".join(file_name.split('.'))\n",
    "data_file = data_path + file_name + \".pkl\"\n",
    "with open(data_file, 'wb') as handle:\n",
    "    pickle.dump(res, handle)\n",
    "print('Saved to ', data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cycling\n",
      "Loaded from  ../data/neuro_noisy_cycling_n_512.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:02<00:00,  3.56it/s]\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:11<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flipflop\n",
      "Loaded from  ../data/neuro_noisy_flipflop_n_512.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  7.15it/s]\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:05<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mante\n",
      "Loaded from  ../data/neuro_noisy_mante_n_512.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:02<00:00,  4.54it/s]\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:08<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "romo\n",
      "Loaded from  ../data/neuro_noisy_romo_n_512.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  6.60it/s]\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:05<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complex_sine\n",
      "Loaded from  ../data/neuro_noisy_complex_sine_n_512.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [00:02<00:00,  4.57it/s]\n",
      "100%|█████████████████████████████████████████████| 6/6 [00:08<00:00,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to  ../data/neuro_dissimilarity_no_noise.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute dissimilarity\n",
    "# Takes about 1 min\n",
    "\n",
    "### Dissimilarity metrics: Ridge CCA and co.\n",
    "alpha_ridge_CCA = 1\n",
    "dist_eucl_sce = []\n",
    "dist_ang_sce = []\n",
    "svs_cca_sce = []\n",
    "dist_ex_eucl_sce = []\n",
    "dist_ex_ang_sce = []\n",
    "\n",
    "for i_f, file_name in enumerate(file_names):\n",
    "    data_file = data_path + file_name\n",
    "    task_name = task_names[i_f]\n",
    "    print(task_name)\n",
    "\n",
    "    with open(data_file, 'rb') as handle:\n",
    "        res = pickle.load(handle)\n",
    "    [\n",
    "        n_steps, n_samples, gs, out_scales, n_sce, opt_gens, lr0s, n_mi, dim_hid, dim_in, dim_out, \n",
    "        dt, rec_step_dt, n_layers, bias, train_in, train_hid, train_out, train_layers, nonlin, \n",
    "        gaussian_init, h_0_std, noise_input_std, noise_init_std, noise_hid_std, batch_size, \n",
    "        task_params, task_params_ev, n_t_ev, task_ev, n_if, n_ifn, steps, loss_all, \n",
    "        output_all, hids_all, \n",
    "        h_0_all, sd_if_all, \n",
    "    ] = res[:38]\n",
    "    del res\n",
    "    print('Loaded from ', data_file)\n",
    "\n",
    "    # Only keep the hidden states necessary\n",
    "    if with_noise:\n",
    "        # Dynamics with the noise used during training.\n",
    "        hids_init_all = hids_all[0]\n",
    "        hids_final_all = hids_all[1]\n",
    "    else:\n",
    "        # Noise-free testing dynamics: i_ifn = 2, 3\n",
    "        hids_init_all = hids_all[2]\n",
    "        hids_final_all = hids_all[3]\n",
    "    del hids_all\n",
    "\n",
    "    # Grace period\n",
    "    n_t_min = int(t_pc_min / (dt * rec_step_dt))\n",
    "    \n",
    "    ################################################################################\n",
    "    # Generalized cos similarity\n",
    "    # Within class. There is no specific order between samples. Unfold upper triangle.\n",
    "    i_s_is, i_s_js = np.triu_indices(n_samples, k=1)\n",
    "    n_triu = len(i_s_is)\n",
    "    dist_eucl = np.zeros((n_triu, n_sce))\n",
    "    dist_ang = np.zeros((n_triu, n_sce))\n",
    "    svs_cca = np.zeros((n_triu, n_sce, dim_hid))\n",
    "    for i_triu in tqdm(range(n_triu)):\n",
    "        i_s_i = i_s_is[i_triu]\n",
    "        i_s_j = i_s_js[i_triu]\n",
    "        for i_sce in range(n_sce):\n",
    "            mi = i_s_i, i_sce\n",
    "            mj = i_s_j, i_sce\n",
    "            # Hidden states\n",
    "            h_i = hids_final_all[mi][:, n_t_min:].reshape(-1, dim_hid)\n",
    "            h_j = hids_final_all[mj][:, n_t_min:].reshape(-1, dim_hid)\n",
    "            d_eucl, d_ang, U, S, VT, norm_X_t, norm_Y_t = ridge_CCA(h_i, h_j, alpha_ridge_CCA)\n",
    "            # Save\n",
    "            mij = i_triu, i_sce\n",
    "            dist_eucl[mij] = d_eucl\n",
    "            dist_ang[mij] = d_ang\n",
    "            svs_cca[mij] = S / (norm_X_t * norm_Y_t)\n",
    "\n",
    "    # Between class\n",
    "    i_sce_ex, j_sce_ex = np.triu_indices(n_sce, k=1)\n",
    "    n_triu_ex = len(i_sce_ex)\n",
    "    dist_ex_eucl = np.zeros((n_triu_ex, n_samples, n_samples))\n",
    "    dist_ex_ang = np.zeros((n_triu_ex, n_samples, n_samples))\n",
    "    for i_triu in tqdm(range(n_triu_ex)):\n",
    "        i_sce = i_sce_ex[i_triu]\n",
    "        j_sce = j_sce_ex[i_triu]\n",
    "        for i_s, j_s in np.ndindex((n_samples, n_samples)):\n",
    "            m_i = i_s, i_sce\n",
    "            m_j = j_s, j_sce\n",
    "            # Hidden states\n",
    "            h_i = hids_final_all[m_i][:, n_t_min:].reshape(-1, dim_hid)\n",
    "            h_j = hids_final_all[m_j][:, n_t_min:].reshape(-1, dim_hid)\n",
    "            # Ridge CCA\n",
    "            d_eucl, d_ang, WX, S, WY, norm_X_t, norm_Y_t = ridge_CCA(h_i, h_j, alpha_ridge_CCA)\n",
    "            # Save\n",
    "            mij = i_triu, i_s, j_s\n",
    "            dist_ex_eucl[mij] = d_eucl\n",
    "            dist_ex_ang[mij] = d_ang\n",
    "\n",
    "    # Clear memory\n",
    "    del hids_init_all, hids_final_all\n",
    "\n",
    "    #################################################################################\n",
    "    # Join\n",
    "    dist_eucl_sce.append(dist_eucl)\n",
    "    dist_ang_sce.append(dist_ang)\n",
    "    svs_cca_sce.append(svs_cca)\n",
    "    dist_ex_eucl_sce.append(dist_ex_eucl)\n",
    "    dist_ex_ang_sce.append(dist_ex_ang)\n",
    "    \n",
    "lbl_sce = [r\"$g=%.1f$, $\\sigma^{(0)}$ %s\" % (gs[i_sce], out_scales[i_sce]) \n",
    "           for i_sce in range(n_sce)]\n",
    "res = [\n",
    "    task_names, task_lbls, n_task, lbl_sce,\n",
    "    n_samples, n_sce, n_mi, dim_hid, \n",
    "    with_noise, \n",
    "    alpha_ridge_CCA, dist_eucl_sce, dist_ang_sce, svs_cca_sce, \n",
    "    dist_ex_eucl_sce, dist_ex_ang_sce, \n",
    "]\n",
    "file_name = \"neuro_dissimilarity\"\n",
    "if not with_noise:\n",
    "    file_name += \"_no_noise\"\n",
    "file_name = \"_\".join(file_name.split('.'))\n",
    "data_file = data_path + file_name + \".pkl\"\n",
    "with open(data_file, 'wb') as handle:\n",
    "    pickle.dump(res, handle)\n",
    "print('Saved to ', data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Perturbations\n",
    "# This can take 40 min. \n",
    "\n",
    "# Perturbation directions\n",
    "pert_dirs = ['w_out', 'pc', 'rand', 'w_in']\n",
    "pert_dir_lbls = [\n",
    "    r'$\\mathbf{w}_\\mathrm{out}$', \n",
    "    r'PCs', \n",
    "    r'rand', \n",
    "    r'$\\mathbf{w}_\\mathrm{in}$', \n",
    "]\n",
    "n_pd = len(pert_dirs)\n",
    "\n",
    "# Perturbation amplitudes\n",
    "n_pa = 21\n",
    "pert_amp_maxs = {\n",
    "    'cycling': 10, \n",
    "    'flipflop': 10, \n",
    "    'mante': 10, \n",
    "    'romo': 5,\n",
    "    'complex_sine': 10, \n",
    "}\n",
    "# Minimal time at which perturbations start\n",
    "t_pert_mins = {\n",
    "    'cycling': 5, \n",
    "    'flipflop': 5, \n",
    "    'mante': 10, \n",
    "    'romo': 1,\n",
    "    'complex_sine': 5, \n",
    "}\n",
    "# Length of interval on which perturbations happen. \n",
    "# This is relevant only for tasks in which the decision time starts right away, because we use this duration to \n",
    "# shift the loss eval mask.\n",
    "dt_pert_intvls = {\n",
    "    'cycling': 10, \n",
    "    'flipflop': 5, \n",
    "    'complex_sine': 20, \n",
    "}\n",
    "\n",
    "# Perturbation time: \n",
    "# There should be a minimal distance to the decision period.\n",
    "# Either, the decision starts late enought so that there's ample time for decision (Mante, Romo)\n",
    "# Or the decision starts almost immediately (sine, cycling). In this case, we shall delay the decision time. \n",
    "# Minimal difference between perturbation and decision (= start of loss evaluation).\n",
    "dt_pert_loss = 5\n",
    "# Number of different perturbation times\n",
    "n_pt = 10\n",
    "\n",
    "# Joint indices\n",
    "n_mip = n_pd, n_pa, n_pt\n",
    "\n",
    "# Number of samples\n",
    "batch_size_pert = 32\n",
    "\n",
    "# Number of PCs and output vectors from which to draw perturbation direction\n",
    "n_comp_pert = 2\n",
    "\n",
    "# Results arrays\n",
    "resp_lbls = [r\"short\", r\"long\", r\"loss\"]\n",
    "n_resp = len(resp_lbls)\n",
    "\n",
    "#######################################################################################\n",
    "# Results arrays\n",
    "n_samples = 5\n",
    "n_sce = 4\n",
    "n_mi = n_samples, n_sce\n",
    "loss_pre_task = torch.zeros((n_task, *n_mi))\n",
    "loss_pert_task = torch.zeros((n_task, *n_mi, *n_mip))\n",
    "output_pre_task = []\n",
    "output_pert_task = []\n",
    "task_pert_task = []\n",
    "wo_proj_pw_task = []\n",
    "h_pre_proj_pw_task = []\n",
    "h_pert_proj_pw_task = []\n",
    "t_perts_task = torch.zeros((n_task, n_pt))\n",
    "\n",
    "# Iterate over tasks\n",
    "# for i_task in [4]:\n",
    "for i_task in range(n_task):\n",
    "    task_name = task_names[i_task]\n",
    "    file_name = file_names[i_task]\n",
    "    data_file = data_path + file_name\n",
    "    print(task_name)\n",
    "    with open(data_file, 'rb') as handle:\n",
    "        res = pickle.load(handle)\n",
    "        [\n",
    "            n_steps, n_samples, gs, out_scales, n_sce, opt_gens, lr0s, n_mi, dim_hid, dim_in, dim_out, \n",
    "            dt, rec_step_dt, n_layers, bias, train_in, train_hid, train_out, train_layers, nonlin, \n",
    "            gaussian_init, h_0_std, noise_input_std, noise_init_std, noise_hid_std, batch_size, \n",
    "            task_params, task_params_ev, n_t_ev, task_ev, n_if, n_ifn, steps, \n",
    "            loss_all, \n",
    "            _, _ , #output_all, hids_all, \n",
    "            h_0_all, sd_if_all, \n",
    "        ] = res[:38]\n",
    "        del res\n",
    "    print('Loaded from ', data_file)\n",
    "\n",
    "    # Task\n",
    "    ts_ex, input_ex, target_ex, mask_ex, noise_input_ex, noise_init_ex = [to_dev(arr) for arr in task_ev]\n",
    "    # Loss for zero output\n",
    "    loss_crit = torch.nn.MSELoss()\n",
    "    loss_0 = loss_crit(target_ex[mask_ex] * 0, target_ex[mask_ex]).item()\n",
    "    task_ex = task_ev\n",
    "    n_t_ex = len(ts_ex)\n",
    "    # Hidden state noise. For initial and input, we use the frozen arrays!\n",
    "    noise_hid_std_ex = 0.\n",
    "\n",
    "    # Min and max time for pca\n",
    "    n_t_pc_min = int(t_pc_min / (dt * rec_step_dt))\n",
    "\n",
    "    # Perturbation amplitudes\n",
    "    pert_amp_max = pert_amp_maxs[task_name]\n",
    "    pert_amps = np.linspace(0, pert_amp_max * np.sqrt(dim_hid), n_pa)\n",
    "\n",
    "    # Perturbation times. First compute the minimal decision times.\n",
    "    t_pert_min = t_pert_mins[task_name]\n",
    "    mask_pert = mask_ex.clone()\n",
    "    target_pert = target_ex.clone()\n",
    "    if task_name in ['cycling', 'flipflop', 'complex_sine']:\n",
    "        # Shift decision to the end of the interval. \n",
    "        #Then allow perturbations in the long interval before, regardless of when the input pulses are given.\n",
    "        # Shift the decision mask, then choose perturbation on freed interval.\n",
    "        t_pert_max = t_pert_min + dt_pert_intvls[task_name]\n",
    "        t_loss_min_i = t_pert_max + dt_pert_loss\n",
    "        mask_pert[:, ts_ex < t_loss_min_i] = False\n",
    "    elif task_name == 'mante':\n",
    "        # The original Mante task demands fixation at zero during the evidence presentation. \n",
    "        # We will remove this part in order to allow for a perturbation meanwhile. \n",
    "        i_tg1 = np.argmax(target_ex != 0, axis=1).min().item()\n",
    "        mask_pert[:, :i_tg1] = False\n",
    "        t0_loss = float(ts_ex[i_tg1])\n",
    "        t_pert_max = t0_loss - dt_pert_loss\n",
    "    elif task_name == 'romo':\n",
    "        # Shift decision to the end of the interval. \n",
    "        #Then allow perturbations in the long interval before, regardless of when the input pulses are given.\n",
    "        i_b_last = torch.Tensor([ts_ex[m_i].min() for m_i in mask_ex[:, :, 0]]).argmax().item()\n",
    "        mask_pert[:] = mask_ex[i_b_last]\n",
    "        target_pert[mask_pert] = target_ex[mask_ex]\n",
    "        t0_loss = float(ts_ex[mask_pert[0, :, 0]].min())\n",
    "        t_pert_max = t0_loss - dt_pert_loss\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "    t_perts = t_pert_min + ((t_pert_max - t_pert_min) / (dt * rec_step_dt) * np.arange(n_pt) // n_pt) * (dt * rec_step_dt)\n",
    "\n",
    "    # Output for saving\n",
    "    output_pre_all = torch.zeros((*n_mi, batch_size_pert, n_t_ex, dim_out))\n",
    "    output_pert_all = torch.zeros((*n_mi, *n_mip, batch_size_pert, n_t_ex, dim_out))\n",
    "    # Projection on 2 PCs and w_out\n",
    "    n_comp_pw = 3\n",
    "    wo_proj_pw_all = torch.zeros((*n_mi, dim_out, n_comp_pw))\n",
    "    h_pre_proj_pw_all = torch.zeros((*n_mi, batch_size_pert, n_t_ex, n_comp_pw))\n",
    "    h_pert_proj_pw_all = torch.zeros((*n_mi, *n_mip, batch_size_pert, n_t_ex, n_comp_pw))\n",
    "\n",
    "    time0 = time.time()\n",
    "    for mi in np.ndindex(*n_mi):\n",
    "        print(mi)\n",
    "        i_s, i_sce = mi\n",
    "        out_scale = out_scales[i_sce]\n",
    "        g = gs[i_sce]\n",
    "        # Network instance\n",
    "        net = RNN_Net(dim_in, dim_hid, dim_out, n_layers, nonlin, bias, out_scale, g, gaussian_init, \n",
    "                      dt, rec_step_dt, train_layers)\n",
    "        net.load_state_dict(sd_if_all[1][mi])\n",
    "        h_0 = h_0_all[mi]\n",
    "        # Transfer\n",
    "        net.to(device)\n",
    "        h_0 = h_0.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Run unperturbed dynamics\n",
    "            output_pre, hids_pre = net.forward_hid(input_ex + noise_input_ex, \n",
    "                                           h_0 + noise_init_ex, \n",
    "                                           noise_hid_std_ex)\n",
    "            output_pre_all[mi] = output_pre.cpu()\n",
    "            loss_pre_task[i_task][mi] = loss_crit(output_pre[mask_pert], target_pert[mask_pert]).item()\n",
    "            # Output weights and PCs\n",
    "            w_in = net.rnn.weight_ih_l0.clone()\n",
    "            w_out = net.decoder.weight.clone()\n",
    "            h = hids_pre[0, :, n_t_pc_min:].reshape(-1, dim_hid)\n",
    "            pca = PCA(n_comp_pert)\n",
    "            pca.fit(h)\n",
    "            pcs = torch.from_numpy(np.float32(pca.components_))\n",
    "\n",
    "            # Leading 2 PCs and output (1st direction).\n",
    "            pcs_w = np.linalg.qr(np.r_[pca.components_[:2], w_out[:1]].T)[0].T\n",
    "            for i_out in range(len(pcs_w)):\n",
    "                if w_out[0] @ pcs_w[i_out] < 0:\n",
    "                    pcs_w[i_out] *= -1 # Fix signs\n",
    "            h_pre_proj_pw_all[mi]= hids_pre @ pcs_w.T\n",
    "            wo_proj_pw_all[mi] = w_out @ pcs_w.T\n",
    "\n",
    "            # Print correlations between output and PCs\n",
    "            corr_wo_pc = w_out @ pcs.T / (\n",
    "                torch.linalg.norm(w_out, axis=-1)[:, None] * \n",
    "                torch.linalg.norm(pcs, axis=-1)[None, :] )\n",
    "\n",
    "            for mip in np.ndindex(*n_mip):\n",
    "                i_pd, i_pa, i_pt = mip\n",
    "                pert_dir = pert_dirs[i_pd]\n",
    "                pert_amp = pert_amps[i_pa]\n",
    "                t_pert = t_perts[i_pt]\n",
    "                i_t_pert = int(t_pert / (dt * rec_step_dt))\n",
    "\n",
    "                # Perturbations\n",
    "                if pert_dir == 'pc':\n",
    "                    pert_vecs = torch.randn((batch_size_pert, n_comp_pert), device=device) @ pcs[:n_comp_pert]\n",
    "                    pert_vecs[0] = pcs[0] # The 0th entry is always along the leading PC\n",
    "                elif pert_dir == 'w_out':\n",
    "                    pert_vecs = torch.randn((batch_size_pert, dim_out), device=device) @ torch.linalg.qr(w_out.T)[0].T\n",
    "                    pert_vecs[0] = w_out[0] / np.linalg.norm(w_out[0])\n",
    "                elif pert_dir == 'rand':\n",
    "                    pert_vecs = torch.randn(batch_size_pert, dim_hid)\n",
    "                elif pert_dir == 'w_in':\n",
    "                    pert_vecs = torch.randn((batch_size_pert, dim_in), device=device) @ torch.linalg.qr(w_in)[0].T\n",
    "                    pert_vecs[0] = w_in[:, 0] / np.linalg.norm(w_in[:, 0])\n",
    "                # Normalize\n",
    "                pert_vecs /= torch.linalg.norm(pert_vecs, axis=-1, keepdims=True)\n",
    "\n",
    "                # Initial state: state at t_pert + perturbation\n",
    "                h_0_pert = hids_pre[:, :, i_t_pert] + pert_amp * pert_vecs[None]\n",
    "                # Run perturbed dynamics. Note: input is not down-sampled by rec_step_dt (in contrast to output, target)\n",
    "                output_pert = output_pre.clone()\n",
    "                hids_pert = hids_pre.clone()\n",
    "                # The model changed a bit. Now we're actually saving h_t and not h_t+1. \n",
    "                # output_pert[:, i_t_pert+1:] = net.forward((input_ex + noise_input_ex)[:, i_t_pert * rec_step_dt + 1:], \n",
    "                output_pert[:, i_t_pert:], hids_pert[:, :, i_t_pert:] = net.forward_hid((input_ex + noise_input_ex)[:, i_t_pert * rec_step_dt:], \n",
    "                                           h_0_pert, \n",
    "                                           noise_hid_std_ex)\n",
    "                output_pert_all[mi][mip] = output_pert.cpu()\n",
    "                # Loss. Note that the mask is adapted, so we also compute the unperturbed loss separately each time.\n",
    "                loss_pert_task[i_task][mi][mip] = loss_crit(output_pert[mask_pert], target_pert[mask_pert]).item()\n",
    "\n",
    "                # Projection on the leading 2 PCs and the remaining direction for the first output vector\n",
    "                h_pert_proj_pw_all[mi][mip] = hids_pert @ pcs_w.T\n",
    "\n",
    "    print(\"Took %.1f sec.\"% (time.time() - time0))\n",
    "    # Save output and task\n",
    "    output_pre_task.append(output_pre_all)\n",
    "    output_pert_task.append(output_pert_all)\n",
    "    wo_proj_pw_task.append(wo_proj_pw_all)\n",
    "    h_pre_proj_pw_task.append(h_pre_proj_pw_all)\n",
    "    h_pert_proj_pw_task.append(h_pert_proj_pw_all)\n",
    "    task_pert = ts_ex, input_ex, target_pert, mask_pert, noise_input_ex, noise_init_ex\n",
    "    task_pert_task.append(task_pert)\n",
    "    t_perts = torch.Tensor(t_perts.astype('float32'))\n",
    "    t_perts_task[i_task] = t_perts\n",
    "\n",
    "    # Save this task (in case saving all goes wrong...)\n",
    "    lbl_sce = [r\"$g=%.1f$, $\\sigma^{(0)}$ %s\" % (gs[i_sce], out_scales[i_sce]) \n",
    "               for i_sce in range(n_sce)]\n",
    "    res = [\n",
    "        task_names, task_lbls, n_task, lbl_sce,\n",
    "        n_samples, n_sce, n_mi, dim_hid, \n",
    "        pert_dirs, pert_dir_lbls, n_pd, n_pa, pert_amp_maxs, t_pert_mins, dt_pert_intvls, dt_pert_loss, n_pt, n_mip, \n",
    "        batch_size_pert, \n",
    "        n_comp_pert, resp_lbls, n_resp, \n",
    "        loss_pre_task, loss_pert_task, \n",
    "        noise_hid_std_ex, \n",
    "        task_pert, t_perts,\n",
    "        output_pre_all, output_pert_all, \n",
    "        wo_proj_pw_all, h_pre_proj_pw_all, h_pert_proj_pw_all,\n",
    "    ]\n",
    "    file_name = \"neuro_perturb_\" + task_name \n",
    "    file_name = \"_\".join(file_name.split('.'))\n",
    "    data_file = data_path + file_name + \".pkl\"\n",
    "    with open(data_file, 'wb') as handle:\n",
    "        pickle.dump(res, handle)\n",
    "    print('Saved to ', data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to  ../data/neuro_perturb.pkl\n"
     ]
    }
   ],
   "source": [
    "# The single datasets may be too large to fit in memory. Remove the traces except for cycling (where we plot the example).\n",
    "\n",
    "# Pytorch\n",
    "task_names = [\n",
    "    \"cycling\",\n",
    "    \"flipflop\", \n",
    "    \"mante\",\n",
    "    \"romo\",\n",
    "    \"complex_sine\",\n",
    "]\n",
    "dim_hid = 512\n",
    "task_lbls = [\" \".join(tn.split(\"_\")) for tn in task_names]\n",
    "task_lbls = [\n",
    "    \"Cycling\",\n",
    "    \"3-bit flipflop\",\n",
    "    \"Mante\",\n",
    "    \"Romo\",\n",
    "    \"Complex sine\",\n",
    "]\n",
    "n_task = len(task_names)\n",
    "\n",
    "n_pd = 4\n",
    "n_pa = 21\n",
    "n_pt = 10\n",
    "n_mip = n_pd, n_pa, n_pt\n",
    "\n",
    "#######################################################################################\n",
    "# Results arrays\n",
    "import torch\n",
    "n_samples = 5\n",
    "n_sce = 4\n",
    "n_mi = n_samples, n_sce\n",
    "loss_pre_task = torch.zeros((n_task, *n_mi))\n",
    "loss_pert_task = torch.zeros((n_task, *n_mi, *n_mip))\n",
    "output_pre_task = []\n",
    "output_pert_task = []\n",
    "task_pert_task = []\n",
    "wo_proj_pw_task = []\n",
    "h_pre_proj_pw_task = []\n",
    "h_pert_proj_pw_task = []\n",
    "t_perts_task = torch.zeros((n_task, n_pt))\n",
    "\n",
    "# Iterate over tasks\n",
    "for i_task in range(n_task):\n",
    "# for i_task in [2]:\n",
    "    task_name = task_names[i_task]\n",
    "    data_file = os.path.join(data_path, \"neuro_perturb_%s.pkl\" % task_name)\n",
    "    with open(data_file, 'rb') as handle:\n",
    "        [\n",
    "                _, _, _, lbl_sce,\n",
    "                n_samples, n_sce, n_mi, dim_hid, \n",
    "                pert_dirs, pert_dir_lbls, n_pd, n_pa, pert_amp_maxs, t_pert_mins, dt_pert_intvls, dt_pert_loss, n_pt, n_mip, \n",
    "                batch_size_pert, \n",
    "                n_comp_pert, resp_lbls, n_resp, \n",
    "                loss_pre_task_i, loss_pert_task_i, \n",
    "                noise_hid_std_ex, \n",
    "                task_pert, t_perts,\n",
    "                output_pre_all, output_pert_all, \n",
    "                wo_proj_pw_all, h_pre_proj_pw_all, h_pert_proj_pw_all,\n",
    "        ] = pickle.load(handle)\n",
    "        \n",
    "    if not task_name == \"cycling\":\n",
    "        output_pre_all = 0\n",
    "        output_pert_all = 0\n",
    "        h_pre_proj_pw_all = 0\n",
    "        h_pert_proj_pw_all = 0\n",
    "        \n",
    "    # Save output and task\n",
    "    output_pre_task.append(output_pre_all)\n",
    "    output_pert_task.append(output_pert_all)\n",
    "    wo_proj_pw_task.append(wo_proj_pw_all)\n",
    "    h_pre_proj_pw_task.append(h_pre_proj_pw_all)\n",
    "    h_pert_proj_pw_task.append(h_pert_proj_pw_all)\n",
    "    task_pert_task.append(task_pert)\n",
    "    t_perts_task[i_task] = t_perts\n",
    "    \n",
    "    i_task_i = np.where(loss_pre_task_i.std((-2, -1)) != 0)[0]\n",
    "    assert len(i_task_i) == 1\n",
    "    i_task_i = i_task_i[0]\n",
    "    \n",
    "    loss_pre_task[i_task] = loss_pre_task_i[i_task_i]\n",
    "    loss_pert_task[i_task] = loss_pert_task_i[i_task_i]\n",
    "\n",
    "# Save all\n",
    "res = [\n",
    "    task_names, task_lbls, n_task, lbl_sce,\n",
    "    n_samples, n_sce, n_mi, dim_hid, \n",
    "    pert_dirs, pert_dir_lbls, n_pd, n_pa, pert_amp_maxs, t_pert_mins, dt_pert_intvls, dt_pert_loss, n_pt, n_mip, \n",
    "    batch_size_pert, \n",
    "    n_comp_pert, resp_lbls, n_resp, \n",
    "    loss_pre_task, loss_pert_task, \n",
    "    output_pre_task, output_pert_task, \n",
    "    noise_hid_std_ex, task_pert_task, t_perts_task,\n",
    "    wo_proj_pw_task, h_pre_proj_pw_task, h_pert_proj_pw_task,\n",
    "]\n",
    "file_name = \"neuro_perturb\"\n",
    "file_name = \"_\".join(file_name.split('.'))\n",
    "data_file = data_path + file_name + \".pkl\"\n",
    "with open(data_file, 'wb') as handle:\n",
    "    pickle.dump(res, handle)\n",
    "print('Saved to ', data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from  ../data/neuro_noisy_cycling_n_512.pkl\n",
      "Took 18.5 sec.\n",
      "Loaded from  ../data/neuro_noisy_flipflop_n_512.pkl\n",
      "Took 8.4 sec.\n",
      "Loaded from  ../data/neuro_noisy_mante_n_512.pkl\n",
      "Took 13.1 sec.\n",
      "Loaded from  ../data/neuro_noisy_romo_n_512.pkl\n",
      "Took 9.5 sec.\n",
      "Loaded from  ../data/neuro_noisy_complex_sine_n_512.pkl\n",
      "Took 14.2 sec.\n",
      "Saved to  ../data/neuro_noise_compression.pkl\n"
     ]
    }
   ],
   "source": [
    "# Compute Noise compression results\n",
    "# Takes about 2 min.\n",
    "\n",
    "####################################################################################\n",
    "### Variance along directions\n",
    "# Project the difference on the 1st PC and w_perp\n",
    "# Number of tested task conditions\n",
    "n_tc = 4\n",
    "# Number of samples per task condition\n",
    "n_samp_per_tc = 16\n",
    "batch_size_ex = n_tc * n_samp_per_tc\n",
    "n_comp_tca = 2\n",
    "pca = PCA(n_comp_tca)\n",
    "# Number of samples along rand, output, PCs\n",
    "n_rps = 2000\n",
    "n_ops = 100\n",
    "n_pps = 100\n",
    "var_rps = np.zeros((n_task, *n_mi, n_rps))\n",
    "var_ops = np.zeros((n_task, *n_mi, n_ops))\n",
    "var_pps = np.zeros((n_task, *n_mi, n_pps))\n",
    "####################################################################################\n",
    "\n",
    "# Load data\n",
    "for i_f, file_name in enumerate(file_names):\n",
    "    data_file = data_path + file_name\n",
    "    task_name = task_names[i_f]\n",
    "    with open(data_file, 'rb') as handle:\n",
    "        [\n",
    "            n_steps, n_samples, gs, out_scales, n_sce, opt_gens, lr0s, n_mi, dim_hid, dim_in, dim_out, \n",
    "            dt, rec_step_dt, n_layers, bias, train_in, train_hid, train_out, train_layers, nonlin, \n",
    "            gaussian_init, h_0_std, noise_input_std, noise_init_std, noise_hid_std, batch_size, \n",
    "            task_params, task_params_ev, n_t_ev, task_ev, n_if, n_ifn, steps, \n",
    "            loss_all, \n",
    "            _, _ , #output_all, hids_all, \n",
    "            h_0_all, sd_if_all, \n",
    "        ] = pickle.load(handle)[:38]\n",
    "    print('Loaded from ', data_file)\n",
    "\n",
    "    # Task\n",
    "    ts_ex, input_ex, target_ex, mask_ex, noise_input_ex, noise_init_ex = [to_dev(arr) for arr in task_ev]\n",
    "    # Loss for zero output\n",
    "    loss_0 = torch.nn.MSELoss()(target_ex[mask_ex] * 0, target_ex[mask_ex]).item()\n",
    "    n_t_ex = len(ts_ex)\n",
    "    # Noise (if not using the pre-defined arrays anyways)\n",
    "    noise_input_std_ex = noise_input_std\n",
    "    noise_init_std_ex = noise_init_std\n",
    "    noise_hid_std_ex = noise_hid_std\n",
    "\n",
    "    # Adjust task to repeated conditions\n",
    "    input_ex = input_ex[:n_tc].repeat((n_samp_per_tc, 1, 1))\n",
    "    target_ex = target_ex[:n_tc].repeat((n_samp_per_tc, 1, 1))\n",
    "    mask_ex = mask_ex[:n_tc].repeat((n_samp_per_tc, 1, 1))\n",
    "    noise_input_ex = noise_input_std * np.float32(np.random.randn(*input_ex.shape)) / np.sqrt(dt)\n",
    "    noise_init_ex = noise_init_std * np.float32(np.random.randn(n_layers-1, batch_size_ex, dim_hid)) \n",
    "\n",
    "    time0 = time.time()\n",
    "    for mi in np.ndindex(*n_mi):\n",
    "        # print(mi)\n",
    "        i_s, i_sce = mi\n",
    "        out_scale = out_scales[i_sce]\n",
    "        g = gs[i_sce]\n",
    "        # Network instance\n",
    "        net = RNN_Net(dim_in, dim_hid, dim_out, n_layers, nonlin, bias, out_scale, g, gaussian_init, \n",
    "                      dt, rec_step_dt, train_layers)\n",
    "        net.load_state_dict(sd_if_all[1][mi])\n",
    "        h_0 = h_0_all[mi]\n",
    "        h_0 = h_0[:, :n_tc].repeat(1, n_samp_per_tc, 1)  # adjust to task conds.\n",
    "        # Transfer\n",
    "        net.to(device)\n",
    "        h_0 = h_0.to(device)\n",
    "\n",
    "        w_out = sd_if_all[1][mi]['decoder.weight'].clone()\n",
    "        with torch.no_grad():\n",
    "            # Run full dynamics\n",
    "            output, hids = net.forward_hid(input_ex + noise_input_ex, \n",
    "                                           h_0 + noise_init_ex, \n",
    "                                           noise_hid_std_ex, last_time=False)\n",
    "\n",
    "            # Trial conditioned average and fluctuations\n",
    "            h_tca = torch.zeros((n_tc, n_t_ex, dim_hid))\n",
    "            dh = torch.zeros((batch_size_ex, n_t_ex, dim_hid))\n",
    "            for i_tc in range(n_tc):\n",
    "                h_c = hids[0, i_tc::n_tc]\n",
    "                h_c_m = h_c.mean(axis=-3)\n",
    "                h_tca[i_tc] = h_c_m\n",
    "                dh[i_tc::n_tc] = h_c - h_c_m\n",
    "            h_tca = h_tca.reshape(-1, dim_hid)\n",
    "            dh = dh.reshape((-1, dim_hid))\n",
    "\n",
    "            # PCA of trial-cond. avg\n",
    "            pca.fit(h_tca)\n",
    "            pcs_tca = torch.from_numpy(np.float32(pca.components_))\n",
    "\n",
    "            # Compute variance along random and nonrandom directions\n",
    "            # Random projections\n",
    "            rps = torch.randn((n_rps, dim_hid))#, device=device)\n",
    "            rps = rps / rps.norm(dim=1)[:, None]\n",
    "            var_rps[i_f][mi] = (dh @ rps.T).var(axis=0).cpu().numpy()\n",
    "            # Variance along output directions\n",
    "            w_mix = torch.randn((n_ops, dim_out)) @ torch.linalg.qr(w_out.T)[0].T\n",
    "            w_mix /= torch.linalg.norm(w_mix, axis=-1, keepdims=True)\n",
    "            var_ops[i_f][mi] = (dh @ w_mix.T).var(axis=0).cpu().numpy()\n",
    "            # Variance along the trial-condition averaged PCs\n",
    "            w_mix = torch.randn((n_pps, n_comp_tca)) @ pcs_tca\n",
    "            w_mix /= torch.linalg.norm(w_mix, axis=-1, keepdims=True)\n",
    "            var_pps[i_f][mi] = (dh @ w_mix.T).var(axis=0).cpu().numpy()\n",
    "    print(\"Took %.1f sec.\"% (time.time() - time0))\n",
    "\n",
    "# Save all\n",
    "lbl_sce = [r\"$g=%.1f$, $\\sigma^{(0)}$ %s\" % (gs[i_sce], out_scales[i_sce]) \n",
    "           for i_sce in range(n_sce)]\n",
    "res = [\n",
    "    task_names, task_lbls, n_task, lbl_sce,\n",
    "    dim_hid,\n",
    "    n_samples, n_sce, n_mi, \n",
    "    n_tc, n_samp_per_tc,batch_size_ex, n_comp_tca, n_rps, n_ops, n_pps, var_rps, var_ops, var_pps, \n",
    "    t_pc_min,\n",
    "]\n",
    "\n",
    "file_name = \"neuro_noise_compression\"\n",
    "file_name = file_name.replace(\".\", \"_\")\n",
    "data_file = data_path + file_name + \".pkl\"\n",
    "with open(data_file, 'wb') as handle:\n",
    "    pickle.dump(res, handle)\n",
    "print('Saved to ', data_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
